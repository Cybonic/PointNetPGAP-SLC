

experim_name: 'default'
################################################################################
# training parameters
################################################################################
pretrained_root: 'checkpoints'
n_gpu: 1
tiago-deep: '/home/tiago/Dropbox/research/datasets'
root_ws: 'C:\\Users\\ISR\\phd\\datasets'
deep-MS-7C37: '/home/deep/tiagobarros/datasets'
retrieval:
  top_cand: [1,5,25]
  range_thres: 6

max_points: 10000
aug: True 
memory: "DISK"

train_loader:
  sequence: ['06'] #['02','05','06','08']
  
  ground_truth: 
    pos_range: 10 # Loop Threshold [m]
    neg_range: 50
    num_neg: 20
    num_pos: 1
    warmupitrs: 600 # Number of frames to ignore at the beguinning
    roi: 500 

  fraction: 0.3 
  batch_size: 1       # batch size
  shuffle: True
  #rotation: [180,-45,-45]
  
val_loader:
  dataset: 'kitti'
  sequence: ['06']
  ground_truth: 
    pos_range: 25 # Loop Threshold [m]
    neg_range: 1
    num_neg: 1
    num_pos: 1
    warmupitrs: 600 # Number of frames to ignore at the beguinning
    roi: 500 
  batch_size: 1 #50         # batch size
  workers:  0            # number of threads to get data
  shuffle: False
  #rotation: [180,-45,-45]

modelwrapper:
  type: 'ORCHNet'
  minibatch_size: 10


loss:
  type: 'TripletLoss'
  args:
    margin: 0.5
    metric: 'L2' # [L2,Hinge,cosine,kl_divergence]
    reduction: 'mean'


trainer:
  iter_per_epoch: 1
  epochs: 50
  report_val: 1      # every x epochs, report validation set
  save_period: 1
  log_dir: "saved/"
  save_dir: "checkpoints/"
  monitor: "max recall" # [off, max mIoU]
  val_per_epochs: 1
  eval_metric: 'L2'
  results_dir: "/media/tiago/BIG/Orchards/temp/"


optimizer:
  type: "AdamW" #[RMSprop,Adam,SGD]
  args:
    lr: 0.0001
    weight_decay: 0.00005
  lr_scheduler: "ReduceLROnPlateau"

StepLR:
  step_size: 20 
  gamma: 0.1

CosineAnnealingWarmRestarts:
  T_0: 5
  T_mult: 2
  eta_min: 0.00001 # Min learning rate

ReduceLROnPlateau:
  mode: 'min' #['min','max'] (str)
  factor: 0.01
  patience: 10 # Number of epochs with no improvement after which learning rate will be reduced. 
  min_lr: 0.000001

